{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yanhan-ss/NLP/blob/main/linguisticsai_practical3_2324_solutions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBlNegWf-jWA"
      },
      "source": [
        "# Linguistics and artificial intelligence: practical session 3 **solutions**\n",
        "\n",
        "## Sequence classification with neural networks\n",
        "\n",
        "### Introduction\n",
        "\n",
        "In this practical session, we will explore neural network architectures for\n",
        "sequence classification; specifically, we will look at a number of neural network architectures for sentiment analysis on an English dataset of movie reviews (Maas et al., 2011). This is the same dataset we used in the first practical session on machine learning for NLP.\n",
        "\n",
        "First, download the archive that contains the dataset and unpack it, using the commands below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKywkSax3Z2I"
      },
      "source": [
        "!wget http://www.ccl.kuleuven.be/Courses/linguistics_and_ai/data/review_dataset.tar.gz\n",
        "!tar xzvf review_dataset.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ceJp0eEFRDH9"
      },
      "source": [
        "As a reminder: the dataset we'll be using contains a set\n",
        "of movie reviews in English, extracted from the website\n",
        "[imdb.com](https://www.imdb.com). The dataset consists of the text of the review, as\n",
        "well as a sentiment label (positive or negative). Note that\n",
        "  the original ratings on IMDB range from 0 to 10\n",
        "  stars. We will use binary classification instead. In our dataset,\n",
        "  original reviews of $\\leq 4$ stars are considered negative, while\n",
        "  reviews of $\\geq 7$ stars are considered positive. The training set is\n",
        "divided into a training part (for training, 20000 reviews, i.e. 80%) and\n",
        "test part (for evaluation, 5000 reviews, i.e. 20%). The dataset is\n",
        "balanced, which means positive and negative instances are evenly\n",
        "distributed in both train and test set.\n",
        "\n",
        "First, we'll load the training set and test set. To do so, we'll make use of the `pandas` library, which facilitates data manipulation. Issue the commands below:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9uy4PR9K3jSi"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train_df = pd.read_csv('reviews_train.csv', header=0, delimiter='\\t', quoting=3)\n",
        "\n",
        "mask = np.random.rand(len(train_df)) < 0.8\n",
        "train = train_df[mask]\n",
        "valid = train_df[~mask]\n",
        "\n",
        "test = pd.read_csv('reviews_test.csv', header=0, delimiter='\\t', quoting=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGPMLuDyTIYU"
      },
      "source": [
        "Note that we reserve part of the training set (20%) for validation of our training methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-NFB0QmgYOn"
      },
      "source": [
        "We can now examine the data. Explore the dataset using the commands below. Note that negative sentiment is denoted by `0` and positive sentiment by `1`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-qFU3yc3sXb"
      },
      "source": [
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KaqbbOI53wqS"
      },
      "source": [
        "print(train['review'][4])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1jhx9q-3xdq"
      },
      "source": [
        "\n",
        "\n",
        "For the implementation of neural networks, we will make use of Tensorflow and Keras. Tensorflow is Googleâ€™s general machine learning framework, specifically designed for computations involving multi-dimensional arrays, which is particularly useful for neural networks. While the basic tensorflow interface is rather low-level, keras is a high-level library (part of the tensorflow package), that allows us to define neural network architectures in just a few lines of code. Import the modules using the code below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjiSji2VzLJ4"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOW3HkdDWSc5"
      },
      "source": [
        "First, we'll turn our reviews into a numerical representation. Our reviews consist of strings of characters, but a neural network architecture needs numbers to process the data.\n",
        "\n",
        "To do so, we'll make use of `Keras`'s tokenizer, which turns each word into a word index. We first construct its vocabulary by fitting it on the training data; `max_features` defines how many word features will be taken into account (i.e. the size of our vocabulary)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHos8ahaWuOQ"
      },
      "source": [
        "# parameters of our model\n",
        "max_features = 5000\n",
        "\n",
        "tokenizer = keras.preprocessing.text.Tokenizer(num_words=max_features,\n",
        "                                               lower=True,\n",
        "                                               split=\" \")\n",
        "tokenizer.fit_on_texts(train['review'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U5tZaEQ8XcdE"
      },
      "source": [
        "Once the vocabulary is constructed, we can apply our tokenizer to the data, and turn each word into its numerical index. We also construct our label vectors, which contain the class label (0 for negative, and 1 for positive sentiment)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZTvZKOYXTCt"
      },
      "source": [
        "x_train_seq = tokenizer.texts_to_sequences(train['review'])\n",
        "y_train = train[\"sentiment\"]\n",
        "\n",
        "# same for the validation set\n",
        "x_valid_seq = tokenizer.texts_to_sequences(valid['review'])\n",
        "y_valid = valid[\"sentiment\"]\n",
        "\n",
        "#same for test set\n",
        "x_test_seq = tokenizer.texts_to_sequences(test['review'])\n",
        "y_test = test[\"sentiment\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqriA_CNYYos"
      },
      "source": [
        "The following command prints the first sequence, which indicates we indeed have numerical indices instead of words:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pTd4fhDqYF60"
      },
      "source": [
        "print(x_train_seq[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Sk1t5yC4iT0"
      },
      "source": [
        "### Feed-forward neural network\n",
        "\n",
        "First, we will create a feedforward network architecture that uses a very simple feature representation: each document\n",
        "is represented by a vector that represents whether a word is present or not present within that document. The entire feature set consists of our vocabulary (i.e. the 5000 words we defined before). This means that each document is represented by a vector of 5000 features, which indicate whether a word is present (1) or not present (0) in the document. We can create these feature vectors with the tokenizer's `sequences_to_matrix` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OngRqDIPDR_X"
      },
      "source": [
        "x_train_matrix = tokenizer.sequences_to_matrix(x_train_seq)\n",
        "x_valid_matrix = tokenizer.sequences_to_matrix(x_valid_seq)\n",
        "x_test_matrix = tokenizer.sequences_to_matrix(x_test_seq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_o4PoFuaftB"
      },
      "source": [
        "When we print out the matrix, we can see it only contains 0s and 1s. And the matrix shape indicates that we have about 16,0000 vectors (the size of our training set, without the part reserverd for validation) that each contain 5000 values (the size of our vocabulary)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VtIhAkBCEGof"
      },
      "source": [
        "print(x_train_matrix)\n",
        "print('matrix shape: ', x_train_matrix.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUGrUn2G7eeA"
      },
      "source": [
        "We can now construct our feedforward neural network with `keras`. Each of\n",
        "the 5000-valued input vectors is presented to the network, so our input layer needs to\n",
        "be of size `max_features`. The input vectors are then fed forward through the\n",
        "network, into the first hidden layer (using a first weight matrix) of\n",
        "size 4. This creates a hidden vector of 4 values, to which an\n",
        "activation function is applied (a rectified linear unit, to be\n",
        "  precise; intuitively, this function turns the neuron on or off,\n",
        "  according to the inputs it receives). The hidden vector\n",
        "representation is then fed forward to the output layer (using a second\n",
        "weight matrix). A softmax layer is applied to the output values, in\n",
        "order to ensure that the output of the network is a probability\n",
        "distribution. Once the network architecture is defined, we can compile the architecture for computation (additionally defining a loss function as well as an optimization procedure).\n",
        "\n",
        "Note that we're using Keras's `Sequential` class to construct the neural network, but this name is not related to *input sequences*; it's just a convenient class that allows us to stack neural network layers one after the other."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6GrYEBQ8FaB"
      },
      "source": [
        "# construct a neural network model with\n",
        "# 1) an input layer of max_features neurons\n",
        "# 2) a hidden layer with 4 neurons, with relu activation applied\n",
        "# 3) an output layer of 1 neuron, with a sigmoid function applied\n",
        "#    (output between 0 and 1)\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(4, input_shape=(max_features,), activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# this prints a summary of the network architecture, and an overview\n",
        "# of the number of parameters that are used\n",
        "print(model.summary())\n",
        "\n",
        "# the model uses cross-entropy as a loss function, finds the best\n",
        "# parameters using stochastic gradient descent, and prints accuracy\n",
        "# metrics\n",
        "model.compile(loss='binary_crossentropy', optimizer='sgd',\n",
        "              metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-qB5dmV8KPy"
      },
      "source": [
        "We cycle through the training documents for a number of times (or *epochs*, in neural network jargon): we feed each 5000-valued feature\n",
        "vector constructed for each document, and compare the output of the\n",
        "network to the correct, desired output in our training set. In order\n",
        "to compare the outputs, we use our loss function: when the output of\n",
        "the network is different from the actual label in the training set, we\n",
        "incur a loss, and this loss decreases when the network's output\n",
        "becomes more similar to the actual labels in the training set. The\n",
        "goal of the network is now to set the parameters of the network (the\n",
        "two weight matrices) in such a way that the loss is minimal. Using the\n",
        "gradient of the loss function, we find out in what way we need to\n",
        "change our parameters such that our loss decreases. We take small\n",
        "steps into the right direction, until we find an optimum. This is a\n",
        "process known as *gradient descent*. Technically, the\n",
        "  gradients for each parameter are computed using a process called\n",
        "  *backpropagation*. During training, each input vector is\n",
        "  presented to the network, and propagated forward through the\n",
        "  network layer by layer, up to the output layer. The\n",
        "  output of the network is then compared to the desired output (using\n",
        "  the loss function) and an error value is calculated for each neuron\n",
        "  in the output layer. The error values are then propagated backwards,\n",
        "  starting from the output, until each neuron has an associated error\n",
        "  value (representing the contribution of each neuron to the original\n",
        "  output). These error values are then used to calculate the gradient\n",
        "  of the loss function with respect to the weights in the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sf5t8T_-5sN-"
      },
      "source": [
        "# train the model by feeding it a batch of 8 training samples\n",
        "# at a time. When we get to the end of the training set, we repeat the\n",
        "# process, and we do this 3 times (epochs). We validate our model on\n",
        "# the validation set\n",
        "model.fit(x_train_matrix, y_train, validation_data=(x_valid_matrix, y_valid),\n",
        "          epochs=3, batch_size=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njaPhfc2-6rL"
      },
      "source": [
        "#### Exercise 1\n",
        "\n",
        "* Change the size of the hidden layer; change the number of input\n",
        "  features. How does this affect loss and accuracy?\n",
        "\n",
        "* Stochastic gradient descent (sgd) is one of the simplest methods\n",
        "  to find the optimum in the loss functions; a number of more complex\n",
        "  methods exist that use more complex machinery to find the optimum in\n",
        "  a faster and more stable way. One such example is [`adam`](https://keras.io/api/optimizers/adam/). Change the optimizer to `adam`. How does this change the computations?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uT1gfw4-UWTP"
      },
      "source": [
        "## YOUR CODE HERE\n",
        "\n",
        "# number of input features is changed by adapting the size of the input vector\n",
        "# controlled by max_features\n",
        "max_features = 20000\n",
        "\n",
        "tokenizer = keras.preprocessing.text.Tokenizer(num_words=max_features,\n",
        "                                               lower=True,\n",
        "                                               split=\" \")\n",
        "tokenizer.fit_on_texts(train['review'])\n",
        "\n",
        "x_train_seq = tokenizer.texts_to_sequences(train['review'])\n",
        "y_train = train[\"sentiment\"]\n",
        "\n",
        "# same for the validation set\n",
        "x_valid_seq = tokenizer.texts_to_sequences(valid['review'])\n",
        "y_valid = valid[\"sentiment\"]\n",
        "\n",
        "#same for test set\n",
        "x_test_seq = tokenizer.texts_to_sequences(test['review'])\n",
        "y_test = test[\"sentiment\"]\n",
        "\n",
        "x_train_matrix = tokenizer.sequences_to_matrix(x_train_seq)\n",
        "x_valid_matrix = tokenizer.sequences_to_matrix(x_valid_seq)\n",
        "x_test_matrix = tokenizer.sequences_to_matrix(x_test_seq)\n",
        "\n",
        "# hidden layer size is changed to 16\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Dense(16, input_shape=(max_features,), activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# optimizer is changed to adam\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
        "              metrics=['acc'])\n",
        "\n",
        "model.fit(x_train_matrix, y_train, validation_data=(x_valid_matrix, y_valid),\n",
        "          epochs=3, batch_size=8)\n",
        "\n",
        "# training accuracy improves and our model converges much faster, but we need to\n",
        "# be careful that we don't overfit (good score on training data, but inferior\n",
        "# generalization on the validation set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2y__EVecDh6"
      },
      "source": [
        "When we're done experimenting, we can compute our score on the test set (the first value represents the network's loss, while the second value represents the accuracy on the test set):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIxpaRPaB8eL"
      },
      "source": [
        "model.evaluate(x_test_matrix, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iI7ebHc_Jwt"
      },
      "source": [
        "### Continuous bag of words\n",
        "\n",
        "The feature representation we use above - in which a word is either\n",
        "'on' or 'off' - works well, but it does not allow the network to\n",
        "learn similarity between words. As we've seen during the lectures, we\n",
        "can also make use of embeddings (a rich vector representation for each\n",
        "word, that implicitly captures the similarity between words). This is\n",
        "the approach we'll take here: each individual word is represented by\n",
        "an embedding vector of 50 dimensions. In order to create the\n",
        "representation of a document, we will take all the embeddings of the\n",
        "words that appear in the document, and sum them together (or take\n",
        "their average). Vector summation is done in a pairwise\n",
        "  fashion: all the values on the first dimension are summed together,\n",
        "  all the values on the second dimension are summed together, and so\n",
        "  on. This means the result of our summation will also be a vector of\n",
        "  50 dimensions. So instead of having an input vector of size 5000,\n",
        "we now have an input vector of size 50, that represents the 'average',\n",
        "combined meaning of all the words in the document taken together.\n",
        "\n",
        "Crucially, the neural network will also learn the embeddings during\n",
        "training: the embeddings of the network are also parameters that are\n",
        "optimized according to the loss function.\n",
        "\n",
        "First of all, we need to make sure that all of our sequences have the same length, which allows us to input them to the network architecture as a single batch. We therefore \"pad\" the sequences, which means we create fixed-length\n",
        "vectors and fill up the positions that are not used with zeros, with a maximum length of 200. There is a specific function to do so, called `pad_sequences`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEUwSriM_HVS"
      },
      "source": [
        "maxlen = 200\n",
        "\n",
        "x_train_padded = keras.preprocessing.sequence.pad_sequences(x_train_seq,\n",
        "                                                     maxlen=maxlen)\n",
        "x_valid_padded = keras.preprocessing.sequence.pad_sequences(x_valid_seq,\n",
        "                                                    maxlen=maxlen)\n",
        "x_test_padded = keras.preprocessing.sequence.pad_sequences(x_test_seq,\n",
        "                                                    maxlen=maxlen)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPXxCi882Z_A"
      },
      "source": [
        "The next step is to construct our model. We define a sequential\n",
        "model to which we add the following network layers:\n",
        "\n",
        "* We start off with an embedding layer which maps our indices into\n",
        "embeddings of size `embedding_dims`;\n",
        "* We add a `GlobalAveragePooling1D`, which will average the\n",
        "embeddings of all words in the document;\n",
        "* We project onto a single unit output layer, and squash it with a\n",
        "sigmoid, to predict a value between 0\n",
        "(negative) and 1 (positive)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xkoopTyK2VBa"
      },
      "source": [
        "embedding_dims = 50\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Embedding(max_features, embedding_dims, input_length=maxlen),\n",
        "    keras.layers.GlobalAveragePooling1D(),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Oc2UbIk21sj"
      },
      "source": [
        "Once our model is constructed, we can compile it, and fit it to our training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8prQkOP2zQx"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train_padded, y_train, batch_size=32, epochs=10,\n",
        "          validation_data=(x_valid_padded, y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxfEqwTEUdGu"
      },
      "source": [
        "Similar to the embeddings that we created using the Word2Vec algorithm in the previous practical session, we can explore the embeddings that are induced by our neural network architecture for sentiment classification. First, execute the following commands to extract the embedding layer from the network, and prepare it for the `TensorBoard Projector`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pi8_SCYRdn9x"
      },
      "source": [
        "import os\n",
        "from tensorboard.plugins import projector\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "# construct mappings from words to indices and vice versa\n",
        "w2i = tokenizer.word_index\n",
        "i2w = {i:w for w,i in w2i.items()}\n",
        "i2w[0] = 'PAD'\n",
        "\n",
        "# set up a logs directory for Tensorboard\n",
        "log_dir='/logs/embeddings/'\n",
        "if not os.path.exists(log_dir):\n",
        "    os.makedirs(log_dir)\n",
        "\n",
        "# save vocabulary line by line\n",
        "with open(os.path.join(log_dir, 'metadata.tsv'), \"w\") as f:\n",
        "  for i in range(len(i2w)):\n",
        "    f.write(i2w[i] + '\\n')\n",
        "\n",
        "# save the embedding vectors in a variable\n",
        "weights = tf.Variable(model.layers[0].get_weights()[0])\n",
        "\n",
        "# create a checkpoint for the embeddings\n",
        "checkpoint = tf.train.Checkpoint(embedding=weights)\n",
        "checkpoint.save(os.path.join(log_dir, \"embedding.ckpt\"))\n",
        "\n",
        "# Some more configuration setup\n",
        "config = projector.ProjectorConfig()\n",
        "embedding = config.embeddings.add()\n",
        "embedding.tensor_name = \"embedding/.ATTRIBUTES/VARIABLE_VALUE\"\n",
        "embedding.metadata_path = 'metadata.tsv'\n",
        "projector.visualize_embeddings(log_dir, config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCkOq-Xz342i"
      },
      "source": [
        "Start `Projector` with the following command (load the command a second time if you get an error):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PtL_KzYMBIzP"
      },
      "source": [
        "%tensorboard --logdir /logs/embeddings/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Agrgg22ljkp"
      },
      "source": [
        "Compared to the embedding space we created in the previous practical session on word embeddings (where the words were more or less evenly distribution throughout the vector space), we now have a dichotomous vector space, where words tend to end up in one of the two extremes of the vector space. By searching for words with positive polarity (such as *fantastic*) and words with negative polarity (e.g. *horrible*), you can confirm that these extremes are associated with positive and negative sentiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onxpKUIiGCEG"
      },
      "source": [
        "#### Exercise 2\n",
        "\n",
        "* The cbow architecture we're using above does not contain a hidden layer: it directly feeds the input layer (the average document embedding) to the output layer. Add a hidden layer of 16 neurons and a *relu* activation function. How does this change the results?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uR9VxzJjNDSZ"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "model = keras.Sequential([\n",
        "    keras.layers.Embedding(max_features, embedding_dims, input_length=maxlen),\n",
        "    keras.layers.GlobalAveragePooling1D(),\n",
        "    keras.layers.Dense(16, activation='relu'),\n",
        "    keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train_padded, y_train, batch_size=32, epochs=10,\n",
        "          validation_data=(x_valid_padded, y_valid))\n",
        "\n",
        "# With an extra hidden layer, the performance for our training set improves, but\n",
        "# our model is prone to overfitting: scores on validation set deteriorate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppI7TzhK3AJi"
      },
      "source": [
        "Evaluation on test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WVZuyLNeJoLO"
      },
      "source": [
        "model.evaluate(x_test_padded, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsd1XCeE23je"
      },
      "source": [
        "### Convolutional neural network\n",
        "\n",
        "Next, we'll look at an example of a convolutional architecture. The architecture below has a convolutional layer with 32 filters and a window size of 8, followed by a pooling layer of size 2. The result is fed forward to a standard dense layer of size 10, and the output is computed using the sigmoid function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LFDwAQOQmfNR"
      },
      "source": [
        "model = keras.Sequential([\n",
        "  keras.layers.Embedding(max_features, embedding_dims, input_length=maxlen),\n",
        "  keras.layers.Conv1D(filters=32, kernel_size=8, activation='relu'),\n",
        "  keras.layers.MaxPooling1D(pool_size=2),\n",
        "  keras.layers.Flatten(),\n",
        "  keras.layers.Dense(10, activation='relu'),\n",
        "  keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# Once our model is constructed, we can compile it\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Now we train our model on the training set and validate on the test\n",
        "# set\n",
        "model.fit(x_train_padded, y_train, batch_size=32, epochs=2,\n",
        "          validation_data=(x_valid_padded, y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU2KLAFT5mbp"
      },
      "source": [
        "Evaluation on test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIG5MV2Knk9q"
      },
      "source": [
        "model.evaluate(x_test_padded, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MyJq_2z93APj"
      },
      "source": [
        "### Recurrent neural network\n",
        "\n",
        "Finally, we'll look at a recurrent neural network. The following setup provides a 2-layer bi-directional LSTM architecture (biLSTM). The embeddings are sequentially read by a first layer LSTM (both forward and backward) of size 16, and the outputs of the first layer are provided to a second bi-directional LSTM, also of size 16. The final states of the forward and the backward LSTM are concatenated, and forward to the output layer for the final prediction."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccqk0N7iHhZY"
      },
      "source": [
        "model = keras.Sequential([\n",
        "  keras.layers.Embedding(max_features, embedding_dims, input_length=maxlen),\n",
        "  keras.layers.Bidirectional(keras.layers.LSTM(16, return_sequences=True)),\n",
        "  keras.layers.Bidirectional(keras.layers.LSTM(16)),\n",
        "  keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "print(model.summary())\n",
        "\n",
        "# Once our model is constructed, we can compile it\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "# Now we train our model on the training set and validate on the test\n",
        "# set\n",
        "model.fit(x_train_padded, y_train, batch_size=32, epochs=2,\n",
        "          validation_data=(x_valid_padded, y_valid))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iM-rvZLx8Mx7"
      },
      "source": [
        "Evaluation on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFQngjdtM8DL"
      },
      "source": [
        "model.evaluate(x_test_padded, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rG9Uhe3b8X3Y"
      },
      "source": [
        "Final note: the sentiment classification dataset that we've used in our experiments is fairly easy to solve, so the simplest architectures already perform quite well on it. Using more complex architectures (convolutional and recurrent architectures) will generally pay off for predictions on more complex datasets. The main goal of this practical session is to show that the construction of neural network models for sequence classification is possible in just a few line of codes, and to provide you with the knowledge to start applying neural network models to your own data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N35JVsWmowHZ"
      },
      "source": [
        "### Exercise 3\n",
        "\n",
        "Neural network architecture come with a lot of hyperparameters: number of input features (vocabulary size), embedding size, hidden layer size, number of hidden layers, batch size, optimization method, etc. We've already experimented with some of these parameters in exercises 1 en 2. Experiment with different hyperparameter settings for the different architectures, and try to reach the best possible classification score. Remember that you should always use the validation set to select the best model; you should never optimize on your test, and keep it separate to see how your final model will fare in the real world."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL6nDE49RUqi"
      },
      "source": [
        "### References\n",
        "\n",
        "Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. 2011. [Learning Word Vectors for Sentiment Analysis](https://www.aclweb.org/anthology/P11-1015/). The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011)."
      ]
    }
  ]
}
