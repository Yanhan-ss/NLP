{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yanhan-ss/NLP/blob/main/NLP_Exercise_Session_3_Morphology_and_Syntax.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3: Morphology and Syntax\n",
        "\n",
        "This session is based on lectures 6 and 7 on Morphological Processing and Syntactic Parsing.\n"
      ],
      "metadata": {
        "id": "dUqyXtNCDe1J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Ex.1 Context Free Grammars (CFG)\n",
        "### Practice with CFGs\n",
        "*Note: This paper-only part is optional, and meant as a refresher for CFGs if you are not entirely familiar with them yet. You can skip ahead to the following exercise `Implementing CFGs` if you feel comfortable.*\n",
        "\n",
        "Consider the following context-free grammar:\n",
        "\n",
        "  \\begin{align*}\n",
        "    S & \\rightarrow & NP ~ ~ VP\\\\\n",
        "    NP & \\rightarrow & (D) ~ ~ NOM\\\\\n",
        "    VP & \\rightarrow & V ~ ~ (NP) ~ ~ (NP)\\\\\n",
        "    NOM & \\rightarrow & N\\\\\n",
        "    NOM & \\rightarrow & NOM ~ ~ PP\\\\\n",
        "    VP & \\rightarrow & VP ~ ~ PP\\\\\n",
        "    PP & \\rightarrow & P ~ ~ NP\\\\\n",
        "    X & \\rightarrow & X^{+} ~ ~ CONJ ~ ~ X\n",
        "  \\end{align*}\n",
        "\n",
        "and the following lexicon:\n",
        "\n",
        "  \\begin{align*}\n",
        "    D & \\rightarrow & a, the\\\\\n",
        "    N & \\rightarrow & woman, child, dog, book, couch, banana\\\\\n",
        "    V & \\rightarrow & reads, eats, sleeps, looked\\\\\n",
        "    P & \\rightarrow & on, at\\\\\n",
        "    CONJ & \\rightarrow & and, or\\\\\n",
        "  \\end{align*}\n",
        "\n",
        "#### Exercises:\n",
        "\n",
        "1. Give three well-formed English sentences that the grammar allows. Draw the tree structures that the grammar assigns to them.\n",
        "2. Give a well-formed English sentence that is structurally ambiguous according to the grammar. Draw the two distinct tree structures.  \n",
        "3. Give one sentence that the grammar does not allow, but which is nonetheless a well-formed English sentence.\n",
        "4. Give three sentences which the grammar allows, but which are not well-formed English sentences.\n",
        "5. How could you adapt the grammar so that your sentence in (3) is allowed by it? How could you adapt your grammar so that your sentences in (4) are excluded?\n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "YOkXZ8szEB7h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing CFGs\n",
        "\n",
        "In this exercise, we will implement a small fragment of context free grammar, and see how we can use it to parse sentences. For this exercise, we'll make use of the module `nltk`. Import the module using the code below:"
      ],
      "metadata": {
        "id": "RduwlrZJWnKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.draw.tree import draw_trees"
      ],
      "metadata": {
        "id": "at9naC_oECQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our initial CFG grammar consists of the following rules:\n",
        "\n",
        "```\n",
        "   S -> NP VP\n",
        "   NP -> Det N | N\n",
        "   VP -> V PP | V\n",
        "   PP -> P NP\n",
        "```\n",
        "\n",
        "and the following lexicon:\n",
        "\n",
        "```\n",
        "   N -> \"hamsters\" | \"wheels\" | \"people\" | \"ruins\" | \"building\" | \"children\" |\"ice-cream\" | \"professor\" | \"article\" | \"men\" | \"musketeers\" | \"king\"\n",
        "   V -> \"run\" | \"like\" | \"fell\" | \"writes\" | \"became\" | \"hate\"\n",
        "   P -> \"in\"\n",
        "   Det -> \"all\" | \"many\" | \"some\" | \"every\" | \"each\" | \"an\" | \"three\" | \"one\" | \"the\"\n",
        "```\n",
        "\n",
        "Note that the vertical bar (`|`) denotes a choice, i.e., an `NP` can be rewritten as `Det N` or as `N`. We can load our initial grammar using the following command:"
      ],
      "metadata": {
        "id": "YlchmMF4xuIt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "grammar = nltk.CFG.fromstring(\"\"\"\n",
        "   S -> NP VP\n",
        "   NP -> Det N | N\n",
        "   VP -> V PP | V\n",
        "   PP -> P NP\n",
        "   N -> \"hamsters\" | \"wheels\" | \"people\" | \"ruins\" | \"building\" | \"children\" |\"ice-cream\" | \"professor\" | \"article\" | \"men\" | \"musketeers\" | \"king\"\n",
        "   V -> \"run\" | \"like\" | \"fell\" | \"writes\" | \"became\" | \"hate\"\n",
        "   P -> \"in\"\n",
        "   Det -> \"all\" | \"many\" | \"some\" | \"every\" | \"each\" | \"an\" | \"three\" | \"one\" | \"the\"\n",
        "   \"\"\")"
      ],
      "metadata": {
        "id": "aRB1A5vax2dA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll equally define a parser, which parses a given input string based on the grammar we defined (and returns all of the possible parses):"
      ],
      "metadata": {
        "id": "C13KC2CTyEB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parser = nltk.ChartParser(grammar)"
      ],
      "metadata": {
        "id": "Cp9FIWwNyHHZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now use the parser to parse a given input string:"
      ],
      "metadata": {
        "id": "aQYvgwQQyLD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"all hamsters run in wheels\".split()\n",
        "for tree in parser.parse(sentence):\n",
        "    print(tree)"
      ],
      "metadata": {
        "id": "BiPRZx6eyKmT",
        "outputId": "6b9e2f90-c0f8-4fbb-9850-48a0e7b3fc11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP (Det all) (N hamsters))\n",
            "  (VP (V run) (PP (P in) (NP (N wheels)))))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q svgling\n",
        "import svgling\n",
        "from IPython.display import display"
      ],
      "metadata": {
        "id": "KMCNRiDayQTY",
        "outputId": "87afc4ce-b8f8-4b32-d4c3-25d216316804",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.1/67.1 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now visualize the parse tree using the following command:\n"
      ],
      "metadata": {
        "id": "d-DT3OAgyVJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for tree in parser.parse(sentence):\n",
        "    display(svgling.draw_tree(tree))"
      ],
      "metadata": {
        "id": "8DmxRI7QyW1Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercises:**\n",
        "\n",
        "Consider the following sentences:\n",
        "\n",
        "```\n",
        "    all hamsters run in wheels\n",
        "    many people like linguistics\n",
        "    every building fell in ruins\n",
        "    some children hate ice-cream\n",
        "    each professor writes an article\n",
        "    three men became musketeers\n",
        "    one man became the king\n",
        "```\n",
        "\n",
        "\n",
        " 1. Try to parse these sentences, and make sure they are adequately covered by the grammar. Adapt the grammar when necessary.\n",
        "\n",
        " Note: you can either adapt the grammar in the original code, or you can copy the original code and adapt it in the cell below. In any case, note that you will need to rerun both the command that defines the grammar (`nltk.CFG.fromstring`) and the command that defines the parser (`nltk.ChartParser`) in order to properly load your adapted grammar.\n"
      ],
      "metadata": {
        "id": "jMl3vUE9yi0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "ufeHQpI7zmCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " 2. Try parsing the following ungrammatical sentence:\n",
        "\n",
        "```\n",
        "    * many people like\n",
        "```    \n",
        "\n",
        "> How can we mediate the grammar such that it prohibits this kind of ungrammatical sentences?"
      ],
      "metadata": {
        "id": "Ll7Od5dUz1yK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "_4fEw6TN0Swa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ex.2 Graph-based Dependency Parsing\n",
        "\n",
        "(Paper-only exercise)\n"
      ],
      "metadata": {
        "id": "fs-Ttts_D3Yu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Graph-based dependency parsers search through the space of possible trees for a given sentence for a tree (or trees) that maximises some score. Given a sentence $S$ we start by creating a graph $G$ which is a fully-connected, weighted, directed graph where the vertices are the input words and the directed edges represent all possible head-dependent assignments. We’ll include an additional ROOT node with outgoing edges directed at all of the other vertices. The weights of each edge in G reflect the score for each possible head-dependent relation assigned by some scoring algorithm.\n",
        "\n",
        "The best dependency tree for $S$ is the **maximum spanning tree**, or the tree spanning over graph $G$, starting from the ROOT that is a subset of $G$ and covers all vertices in $G$ which has the maximum score.\n",
        "\n",
        "To find such a tree, we can use the Chu and Liu and Edmonds algorithm (CLE). See a detailed explanation on pages 16 and 17 of Chapter 19 of the SLP textbook.\n",
        "\n"
      ],
      "metadata": {
        "id": "85jKtyvR1MxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**: Consider the following sentence:\n",
        "> The cat sees the tree with the binoculars.\n",
        "\n",
        "There is an ambiguity in the meaning of the sentence. We will create a dependency parse of this sentence from the following scores given by a model. Use the CLE algorithm to find the maximum spanning tree and find the optimum parse tree for the sentence. Which of the two possible meanings does this parse correspond to?\n",
        "\n",
        "![image](https://github.com/LAGoM-NLP/NLP-data-repo/blob/0943f185400e47225bc8b04a23c07e57528f86e6/tree.png?raw=true)\n",
        "\n"
      ],
      "metadata": {
        "id": "JlzdgYdiMr28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you are interested in learning about how to implement neural models that can be used to obtain the edge scores above, see this tutorial on biaffine graph-based dependency parsing: https://www.cse.chalmers.se/~richajo/nlp2019/l7/Biaffine%20dependency%20parsing.html"
      ],
      "metadata": {
        "id": "y7S4rtHXA-aE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ex.3 Using spaCy\n",
        "\n",
        "In this exercise we will introduce the spaCy library that is designed to implement natural language processing pipelines. It can be used to preprocess text with sentence segmentation, tokenization, lemmatization (as we have seen in earlier exercises). It is also very efficient in carrying out simple NLP tasks like POS tagging, NER, and dependency parsing. Let's install and import the package:"
      ],
      "metadata": {
        "id": "_i-WdS_u0jnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q spacy\n",
        "import spacy"
      ],
      "metadata": {
        "id": "JH4hWmBX0ldA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use spaCy to preprocess some English text (although it is available in many other languages). The library contains many trained models that have to be loaded before we can process anything. Let's load a model for English (you can also try others in : https://spacy.io/models/en):"
      ],
      "metadata": {
        "id": "J_Bh5t276a55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "wb35Q08i6sEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now process a string of characters using Spacy's preprocessing pipeline. The code below first puts a text string in the variable `text`; this string is then processed by Spacy's pipeline, such that the variable `doc` contains the text augmented with Spacy's annotations."
      ],
      "metadata": {
        "id": "_ezkn_uF7SwE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"The cat sees the tree with the binoculars.\"\n",
        "doc = nlp(text)\n",
        "print(doc)\n",
        "for token in doc:\n",
        "  print(token.text, token.lemma_, token.pos_)"
      ],
      "metadata": {
        "id": "NZU66PG37T8z",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Morphological Analysis\n",
        "\n",
        "The spaCy module can be very useful for morphological analyses of text.\n",
        "\n",
        "[Universal Dependencies (UD)](https://universaldependencies.org/) is a project for creating universal and comparable gold standard treebanks across languages. UD contains consistent linguistic annotations such as parts of speech, syntactic dependencies and morphological features. It has enabled much cross-lingual and multilingual NLP research.\n",
        "UD contains sentences annotated with morphological tags that can be used for morphological analyses. We invite you to go through the [UD annotation guidelines](https://universaldependencies.org/u/overview/morphology.html) for morphologial annotations to understand what they look like. We will learn how to use these annotations with spaCy (based on https://spacy.io/usage/linguistic-features#morphology).\n",
        "\n"
      ],
      "metadata": {
        "id": "dqmcJM6uNHu6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(token, token.morph)"
      ],
      "metadata": {
        "id": "mG6sQ5v-Ejda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc[0].morph.get('PronType') #get the 'PronType' for the 1st element in doc"
      ],
      "metadata": {
        "id": "5sCiOMVoF0tA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The [Morphologizer](https://spacy.io/api/morphologizer) is the pipeline component of spaCy that handles morphological tagging. It is trainable, which means that you can train it on your own morphologically tagged dataset to create your own custom morphological analyser. You can explore the documentation linked here for details if interested."
      ],
      "metadata": {
        "id": "aYsxbREj9yGW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dependency Parsing\n",
        "\n"
      ],
      "metadata": {
        "id": "tsbCuX5DOlaI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The spaCy module also contains a dependency parsing component that applies a transition-based parser to its trained statistical and neural models to generate dependency annotations. They can be accessed as follows:"
      ],
      "metadata": {
        "id": "qcgYuj994r1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc:\n",
        "  print(token.text, token.pos_, '<-', token.head, token.head.pos_, '(', token.dep_, ')', '| children: ',[child for child in token.children])"
      ],
      "metadata": {
        "id": "Af-MxdGCOnTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use the `.explain` method to understand what each tag means:"
      ],
      "metadata": {
        "id": "TLUXI15D_vKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spacy.explain('PRP$')"
      ],
      "metadata": {
        "id": "h9X0u---YDPw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also visualise your dependency tree in a more human-readable format:"
      ],
      "metadata": {
        "id": "DRUHCixf_5Nb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy"
      ],
      "metadata": {
        "id": "mM6HAH-gXVpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "displacy.render(doc, style = 'dep', jupyter = True, options = {'distance': 90})"
      ],
      "metadata": {
        "id": "OKJQ4qIpXert"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each word can have only one head (only a single arrow can arrive at a word), but a word can have multiple dependents (multiple arrows can start from a word). The root of the sentence (the main verb) does not have a head.\n",
        "\n",
        "You can use the syntactic annotation in order to search for syntactic constructions of interest. As an example, the code below finds nouns together with their determiners:"
      ],
      "metadata": {
        "id": "DzYEqGl2AD5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "det_nouns = []\n",
        "for candidate in doc:\n",
        "    if candidate.dep_ == 'det' and candidate.head.pos_ == 'NOUN':\n",
        "        det_nouns.append((candidate, candidate.head))\n",
        "print(det_nouns)"
      ],
      "metadata": {
        "id": "oLWmB9OPXgT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**: Load the dev set (en_ewt-ud-dev.conllu) of the [UD_English-EWT](https://github.com/UniversalDependencies/UD_English-EWT/tree/master) treebank and process the examples in spaCy.\n",
        ">`!git clone https://github.com/UniversalDependencies/UD_English-EWT.git`\n",
        "\n",
        "Tip: You can use the `pyconll` package (https://pyconll.readthedocs.io/en/latest/starting.html) to easily load and process conllu files.\n",
        "\n",
        "Use the acquired annotations to find:\n",
        ">a. All possible combinations of verbs and subjects in the data. Only extract the verb and the head noun of the subject. For example, in the sentence *the dog of the neighbours sleeps*, extract the tuple *(dog, sleeps)*.\n",
        "\n",
        ">b. All instances where the ROOT of the sentence is not a verb.\n",
        "\n",
        ">c. All combinations of verbs and adjectives in the data, where the adjective appears as modifier to the subject of the verb. As an example, in the sentence *the white dog sleeps*, extract the tuple *(white, sleeps)*.\n",
        "\n",
        "Try the different English models available in the spaCy pipeline to get dependency parse trees for the same sentences. Do you see a difference?\n",
        "\n",
        "OPTIONAL: If you speak another language, you can also try running a similar pipeline for a treebank from another language. Simply load a model for that language from https://spacy.io/models using `spacy.load()`. Links to all UD treebanks are in https://universaldependencies.org/."
      ],
      "metadata": {
        "id": "r6n-ngW2BML_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "R6Ycid2Rc7w7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}