{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yanhan-ss/NLP/blob/main/NLP_Exercise_Session_5_Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaLWSzpLfXO6"
      },
      "source": [
        "# Exercise 5: Machine Translation\n",
        "\n",
        "This session is based on lecture 10 (Machine Translation).\n",
        "\n",
        "Relevant SLP chapters:\n",
        "* Chapter 13\n",
        "* Chapter 10.2 on sampling\n",
        "* Also good to refresh on the Transformer: Chapter 9 and Exercise 2\n",
        "\n",
        "## Exercise: Decoding strategies\n",
        "As discussed in the slides and in the book, neural machine translation generally works using a sequence-to-sequence architecture.\n",
        "This is mostly modeled as an encoder and a decoder: a component that creates an embedding of the input tokens, which is then decoded by the decoder component into target tokens.\n",
        "At every timestep during the decoding, the decoder generates a probability distribution over its vocabulary.\n",
        "In other words, what is the most likely token, given the previous tokens in previous timesteps.\n",
        "There are a lot of choices regarding *how* we handle the output of the decoder:\n",
        "\n",
        "* Do we always pick the token with the highest probability?\n",
        "* Do we do keep alternative options in mind?\n",
        "* Which and how many hypotheses do we keep in mind?\n",
        "* What about memory and efficiency (time) considerations?\n",
        "\n",
        "These types of questions are related to the general problems of *sampling* and *decoding*.\n",
        "This is a fundamental aspect in dealing with (generative) language models (if you've used any LLM and seen a 'temperature' parameter, it's directly related to this decoding process).\n",
        "\n",
        "We are going to look at some decoding strategies, using a small, pre-trained NL $\\rightarrow$ EN encoder-decoder translation model.\n",
        "\n",
        "This is a 'fill in the blank' exercise, the parts for you to implement are marked with `# TODO: <description>`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iIBJMo5gfXO8"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers tqdm torch ipywidgets sentencepiece sacrebleu pandas seaborn  # This will take a bit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CV50XOIrfXO9"
      },
      "outputs": [],
      "source": [
        "import dataclasses\n",
        "import time\n",
        "from copy import deepcopy\n",
        "from pathlib import Path\n",
        "\n",
        "import pandas as pd\n",
        "import sacrebleu\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import tqdm\n",
        "from transformers import AutoModelForSeq2SeqLM, T5Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmgQrTxYfXO-"
      },
      "outputs": [],
      "source": [
        "# Classes to keep track of results and sequences\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class EvaluationResult:\n",
        "    bleu: float\n",
        "    chrf: float\n",
        "    chrf_pp: float\n",
        "\n",
        "\n",
        "@dataclasses.dataclass\n",
        "class ScoredToken:\n",
        "    \"\"\"\n",
        "    Represents a token and a corresponding score, which should roughly translate to the likelihood this\n",
        "    token should be appended to some given generation sequence.\n",
        "    \"\"\"\n",
        "\n",
        "    token_id: int\n",
        "    score: float\n",
        "\n",
        "\n",
        "class GeneratedSequence:\n",
        "    \"\"\"\n",
        "    Represents a sequence in the process of being generated; an initial token, a potential end token, and a series of\n",
        "    ScoredTokens between them. This class also maintains the overall sequence score, which is the cumulative probability\n",
        "    of this generated sequence being the best output given some query.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, initial_token, end_token_id, initial_score):\n",
        "        self.end_token_id = end_token_id\n",
        "        self._score = initial_score  # Cumulative log probs of this sequence\n",
        "        self.normalized_score = initial_score\n",
        "        self.sequence = [ScoredToken(initial_token, initial_score)]\n",
        "\n",
        "    def append(self, scored_token):\n",
        "        \"\"\"\n",
        "        Append the given ScoredToken to this sequence; add its log-probability to this\n",
        "        sequence's total cumulative log-prob\n",
        "        \"\"\"\n",
        "        self.sequence.append(scored_token)\n",
        "        self._score += scored_token.score\n",
        "        self.normalized_score = self._score / len(self.sequence)\n",
        "\n",
        "    def ids(self):\n",
        "        return [st.token_id for st in self.sequence]\n",
        "\n",
        "    def has_ended(self):\n",
        "        \"\"\"\n",
        "        Returns True if the last token in this sequence is the end-of-sequence token ID\n",
        "        \"\"\"\n",
        "        return self.sequence and self.sequence[-1].token_id == self.end_token_id\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"{self._score: .6f}({self.normalized_score: .6f}): {self.sequence}\"\n",
        "\n",
        "    def __repr__(self):\n",
        "        return self.__str__()\n",
        "\n",
        "    def __copy__(self):\n",
        "        gs = GeneratedSequence(None, self.end_token_id, 0.0)\n",
        "        gs.sequence = self.sequence.copy()\n",
        "        gs._score = self._score\n",
        "        gs.normalized_score = self.normalized_score\n",
        "        return gs\n",
        "\n",
        "    # We need the methods below to be able to sort instances of this class.\n",
        "    def __iter__(self):\n",
        "        return self.sequence.__iter__()\n",
        "\n",
        "    def __lt__(self, other_sequence):\n",
        "        return self.normalized_score < other_sequence.normalized_score\n",
        "\n",
        "    def __le__(self, other_sequence):\n",
        "        return self.normalized_score <= other_sequence.normalized_score\n",
        "\n",
        "    def __eq__(self, other_sequence):\n",
        "        return self.normalized_score - other_sequence.normalized_score <= 1e-5 and self.ids() == other_sequence.ids()\n",
        "\n",
        "    def __ne__(self, other_sequence):\n",
        "        return self.normalized_score - other_sequence.normalized_score > 1e-5 or self.ids() != other_sequence.ids()\n",
        "\n",
        "    def __gt__(self, other_sequence):\n",
        "        return self.normalized_score > other_sequence.normalized_score\n",
        "\n",
        "    def __ge__(self, other_sequence):\n",
        "        return self.normalized_score >= other_sequence.normalized_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3wI8FSefXO-"
      },
      "outputs": [],
      "source": [
        "# Some convenience functions\n",
        "\n",
        "\n",
        "def detect_device():\n",
        "    \"\"\"Choose the device to run the model on.\"\"\"\n",
        "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def evaluate(hypotheses, references):\n",
        "    \"\"\"Evaluate hypotheses generated by a model with the gold references.\"\"\"\n",
        "    bleu = sacrebleu.corpus_bleu(hypotheses, [references]).score\n",
        "    chrf = sacrebleu.corpus_chrf(hypotheses, [references]).score\n",
        "    chrf_pp = sacrebleu.corpus_chrf(\n",
        "        hypotheses,\n",
        "        [references],\n",
        "        word_order=2,\n",
        "        char_order=6,\n",
        "        beta=2,\n",
        "        eps_smoothing=False,\n",
        "    ).score\n",
        "\n",
        "    return EvaluationResult(bleu, chrf, chrf_pp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqxPsHAYfXO-"
      },
      "outputs": [],
      "source": [
        "def greedy_search(\n",
        "    model,\n",
        "    src_input_ids: torch.Tensor,\n",
        "    start_token_id: int,\n",
        "    end_token_id: int,\n",
        "    device: torch.device,\n",
        "    max_length: int = 64,\n",
        "):\n",
        "    \"\"\"\n",
        "    Performs greedy search over the output from model.\n",
        "    At every time step, this procedure will pick the token with\n",
        "    the highest (log) probability and continue to the next time step.\n",
        "    \"\"\"\n",
        "    # The output sequence starts as a tensor containing just the start token ID\n",
        "    output_sequence = torch.full((1, 1), start_token_id, device=device)\n",
        "\n",
        "    for _ in range(max_length - 1):\n",
        "        # Predict next token, given input context IDs and output sequence so far\n",
        "        output = model(input_ids=src_input_ids, decoder_input_ids=output_sequence)\n",
        "\n",
        "        # TODO: your code here to do greedy decoding:\n",
        "        #   1. Extract the raw model output logits\n",
        "        #   2. Extract the argmax, the highest-scoring element, from the model output logits.\n",
        "        #   3. Concatenate the next token ID into the output sequence\n",
        "        #   4. If the next token happens to be the end-of-sequence token, break the loop\n",
        "\n",
        "    # Return greedily-computed output token sequence\n",
        "    return output_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIavTLdYfXO_"
      },
      "outputs": [],
      "source": [
        "def beam_search(\n",
        "    tokenizer,\n",
        "    model,\n",
        "    src_input_ids: torch.Tensor,\n",
        "    start_token_id: int,\n",
        "    end_token_id: int,\n",
        "    device: torch.device,\n",
        "    beam_width: int = 8,\n",
        "    temperature: int = 1.0,\n",
        "    max_length: int = 64,\n",
        "    verbose: bool = True,\n",
        ") -> list[GeneratedSequence]:\n",
        "    \"\"\"\n",
        "    Performs a beam search to generate heuristically-determined best top-k text sequences from a model.\n",
        "\n",
        "    Args:\n",
        "    tokenizer (Tokenizer): Tokenizer instance used for token manipulation.\n",
        "    model (Model): The model used for generating predictions.\n",
        "    src_input_ids (torch.Tensor): Source input ids for which output needs to be generated.\n",
        "    start_token_id (int): The token id used to start the sequence generation.\n",
        "    end_token_id (int): The token id that indicates the end of a sequence.\n",
        "    device (torch.device): The device (CPU/GPU) where tensors should be allocated.\n",
        "    beam_width (int, optional): The number of sequences to keep at each step of the beam search. Default is 8.\n",
        "    temperature (float, optional): A factor used to model the confidence of the predictions.\n",
        "                                   A higher temperature results in more diversified outputs. Default is 1.0.\n",
        "    max_length (int, optional): The maximum length of the sequence to be generated. Default is 64.\n",
        "    verbose (bool, optional): Flag to log detailed outputs of candidates and beam search process.\n",
        "\n",
        "    Returns:\n",
        "    A list of generated sequences, where each sequence is represented as an object containing sequence ids, score,\n",
        "    and tokenized representation.\n",
        "\n",
        "    Description:\n",
        "    This function initiates a beam search algorithm to generate sequences from a given model. The search begins with a sequence\n",
        "    containing only the start_token_id. At each step, the model predicts the next token for each sequence in the current set of\n",
        "    candidate sequences. The top 'beam_width' tokens and their probabilities are used to extend the current sequences. This process\n",
        "    repeats until the sequences reach the maximum specified length or all sequences end with the end_token_id.\n",
        "    \"\"\"\n",
        "    # The initial candidate sequence is simply the start token ID with a sequence score of 0\n",
        "    # This list will be overwritten after an iteration of further generating the candidates!\n",
        "    candidate_sequences = [GeneratedSequence(start_token_id, end_token_id, 0.0)]\n",
        "\n",
        "    # Build up output sequences until max_length tokens are reached\n",
        "    for _ in tqdm.tqdm(range(max_length - 1), desc=\"Generating candidates...\", disable=not verbose):\n",
        "        # Temporary list to store candidates for the next generation step\n",
        "        next_step_candidates = []\n",
        "\n",
        "        # Iterate through all candidate sequences; for each, generate the next most likely tokens\n",
        "        # and add them to the next-step sequnce of candidates\n",
        "        for candidate in candidate_sequences:\n",
        "            # Append the candidate sequence as-is to the next-step candidates if it already contains an end-of-sequence token\n",
        "            if candidate.has_ended():\n",
        "                next_step_candidates.append(candidate)\n",
        "                continue\n",
        "\n",
        "            # Build a tensor out of the candidate IDs; add a single batch dimension\n",
        "            gen_seq = torch.tensor(candidate.ids(), device=device).unsqueeze(0)\n",
        "            # Predict next token\n",
        "            output = model(input_ids=src_input_ids, decoder_input_ids=gen_seq)\n",
        "\n",
        "            # TODO: your code here for preparing the probabilities:\n",
        "            #   1. Extract logits from output\n",
        "            #   2. Scale logits using temperature value\n",
        "            #   3. Construct probability distribution against scaled logits through softmax activation function\n",
        "            #   4. Select top k (beam_width) probabilities and IDs from the distribution\n",
        "            logits = ...\n",
        "            scaled_logits = ...\n",
        "            probs = ...\n",
        "            top_probs, top_ids = ...\n",
        "\n",
        "            if verbose:\n",
        "                print(f\"{candidate.normalized_score}: [{candidate.ids()}], next token probabilities:\")\n",
        "                for p, w in zip(top_probs.tolist()[0], tokenizer.convert_ids_to_tokens(top_ids.tolist()[0])):\n",
        "                    print(f\"\\tp: {p: .6f}: {w}\")\n",
        "                print(\"\\n\")\n",
        "\n",
        "            # TODO: your code here:\n",
        "            #   For each of the top-k generated tokens, append to this candidate sequence,\n",
        "            #   update its score, and append to next_step_candidates.\n",
        "            #   Make sure to use 'deepcopy' on the candidate to create your new candidate.\n",
        "            for i in range(beam_width):\n",
        "                ...\n",
        "\n",
        "        if verbose:\n",
        "            print(\"Next step candidates:\")\n",
        "            for seq in reversed(sorted(next_step_candidates)):\n",
        "                print(f\"\\t{seq.normalized_score: .6f}: [{seq.tokens()}]\")\n",
        "            print(\"\\n\")\n",
        "\n",
        "        # TODO: your code here:\n",
        "        #   Sort the next-step candidates by their score (descending, best first),\n",
        "        #   select the top-k (beam_width) scoring sequences and make them the new candidate_sequences list.\n",
        "\n",
        "        # Break if all sequences in the heap end with the eos_token_id\n",
        "        if all(seq.has_ended() for seq in candidate_sequences):\n",
        "            if verbose:\n",
        "                print(\"All candidates include EOS, stopping.\")\n",
        "            break\n",
        "\n",
        "    return candidate_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5165a6b65d144b16843f66037a0d7ac8"
          ]
        },
        "id": "y85x_oZrfXO_",
        "outputId": "993e6391-6792-47c1-c2c3-9f092085a1d9"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/134 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5165a6b65d144b16843f66037a0d7ac8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MT5ForConditionalGeneration(\n",
              "  (shared): Embedding(250112, 512)\n",
              "  (encoder): MT5Stack(\n",
              "    (embed_tokens): Embedding(250112, 512)\n",
              "    (block): ModuleList(\n",
              "      (0): MT5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): MT5LayerSelfAttention(\n",
              "            (SelfAttention): MT5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 6)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): MT5LayerFF(\n",
              "            (DenseReluDense): MT5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-7): 7 x MT5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): MT5LayerSelfAttention(\n",
              "            (SelfAttention): MT5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): MT5LayerFF(\n",
              "            (DenseReluDense): MT5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): MT5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (decoder): MT5Stack(\n",
              "    (embed_tokens): Embedding(250112, 512)\n",
              "    (block): ModuleList(\n",
              "      (0): MT5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): MT5LayerSelfAttention(\n",
              "            (SelfAttention): MT5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "              (relative_attention_bias): Embedding(32, 6)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): MT5LayerCrossAttention(\n",
              "            (EncDecAttention): MT5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): MT5LayerFF(\n",
              "            (DenseReluDense): MT5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "      (1-7): 7 x MT5Block(\n",
              "        (layer): ModuleList(\n",
              "          (0): MT5LayerSelfAttention(\n",
              "            (SelfAttention): MT5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (1): MT5LayerCrossAttention(\n",
              "            (EncDecAttention): MT5Attention(\n",
              "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
              "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (2): MT5LayerFF(\n",
              "            (DenseReluDense): MT5DenseGatedActDense(\n",
              "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
              "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "              (act): NewGELUActivation()\n",
              "            )\n",
              "            (layer_norm): MT5LayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (final_layer_norm): MT5LayerNorm()\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (lm_head): Linear(in_features=512, out_features=250112, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "model_id = \"Michielo/mt5-small_nl-en_translation\"\n",
        "tokenizer_id = \"Michielo/mt5-small_nl-en_translation\"\n",
        "\n",
        "device = detect_device()\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(tokenizer_id)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_id)\n",
        "\n",
        "# The model architecture (T5) uses the pad_token_id as the starting token for\n",
        "# text generation (https://huggingface.co/docs/transformers/main/en/model_doc/t5)\n",
        "start_token_id = tokenizer.pad_token_id\n",
        "end_token_id = tokenizer.eos_token_id\n",
        "\n",
        "model.to(device)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LnyerHMIfXO_"
      },
      "outputs": [],
      "source": [
        "# Some parameters to control the beam search behavior\n",
        "beam_width = 3\n",
        "temperature = 1\n",
        "# Maximum token length of generated output\n",
        "max_length = 64\n",
        "# A prompt to append to the input text (we need this to tell the model that the target is English)\n",
        "prompt = \">>en<< \""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJqM22bEfXPA"
      },
      "outputs": [],
      "source": [
        "# Input text to translate\n",
        "source_text = \"Jeetje, wat een lawaai!\"\n",
        "# Prepend a prompt in front of the input text\n",
        "text = f\"{prompt}{source_text}\"\n",
        "\n",
        "# Encode the text into a sequence of IDs\n",
        "input_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Perform the decoding with greedy search\n",
        "greedy_generated_ids = greedy_search(\n",
        "    model,\n",
        "    input_ids,\n",
        "    start_token_id,\n",
        "    end_token_id,\n",
        "    device,\n",
        "    max_length,\n",
        ")\n",
        "# Decode the token IDs back to strings (we skip special tokens since we're not expecting any)\n",
        "greedy_decoded_text = tokenizer.decode(greedy_generated_ids.squeeze(), skip_special_tokens=True)\n",
        "\n",
        "# And now the same, but with beam search\n",
        "beam_generated_options = beam_search(\n",
        "    tokenizer,\n",
        "    model,\n",
        "    input_ids,\n",
        "    start_token_id,\n",
        "    end_token_id,\n",
        "    device,\n",
        "    beam_width,\n",
        "    temperature,\n",
        "    max_length,\n",
        "    verbose=False,  # To print what's happening\n",
        ")\n",
        "# Get a list of all final candidates and decode them into strings\n",
        "all_options = tokenizer.batch_decode([seq.ids() for seq in beam_generated_options], skip_special_tokens=True)\n",
        "beam_options_formatted = \"\\n\".join(\n",
        "    f\"{t} ({s.normalized_score:.5})\" for t, s in zip(all_options, beam_generated_options)\n",
        ")\n",
        "\n",
        "beam_best_candidate = all_options.pop(0)\n",
        "\n",
        "print(f\"\"\"\n",
        "Beam search settings: {beam_width=}, {temperature=}, {max_length=}\n",
        "\n",
        "Input text: {source_text}\n",
        "\n",
        "-- Greedy Result --\n",
        "{greedy_decoded_text}\n",
        "\n",
        "-- Beam Results --\n",
        "{beam_options_formatted}\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dEez-gUfXPA"
      },
      "outputs": [],
      "source": [
        "# Now it's time to evaluate what we have\n",
        "# Remember, our original text was \"Jeetje, wat een lawaai!\"\n",
        "\n",
        "# This gold target is what the translation should be (just an example)\n",
        "gold_target = \"Jeez, what a noise!\"\n",
        "\n",
        "greedy_result = evaluate([greedy_decoded_text], [gold_target])\n",
        "beam_result = evaluate([beam_best_candidate], [gold_target])\n",
        "\n",
        "print(f\"Greedy: {greedy_result}\")\n",
        "print(f\"Beam:   {beam_result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56xneagffXPA"
      },
      "source": [
        "## Checkpoint\n",
        "If you have implemented the decoding strategies correctly, you should have the following output:\n",
        "\n",
        "```\n",
        "Beam search settings: beam_width=3, temperature=1, max_length=64\n",
        "\n",
        "Input text: Jeetje, wat een lawaai!\n",
        "\n",
        "-- Greedy Result --\n",
        "Oh, yeah, that's a noise!\n",
        "\n",
        "-- Beam Results --\n",
        "Oh my gosh, what a noise! (-0.32797)\n",
        "Oh my gosh, that's a noise! (-0.48012)\n",
        "Little noise! (-0.48669)\n",
        "```\n",
        "\n",
        "The evaluation results of the example sentence should be the following:\n",
        "```\n",
        "Greedy: EvaluationResult(bleu=22.089591134157878, chrf=38.157706928433136, chrf_pp=41.06173256467016)\n",
        "Beam:   EvaluationResult(bleu=51.697315395717055, chrf=65.95318577110265, chrf_pp=68.4984585303716)\n",
        "```\n",
        "\n",
        "If your results are not close to these, something is incorrect.\n",
        "If the scores are slightly off, that's fine since floating points can be slightly different between systems.\n",
        "\n",
        "**Make sure you have the correct output before you continue!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S2NIwn7UfXPA"
      },
      "source": [
        "## Exercise 2: Evaluation\n",
        "We have included a small dataset with some Dutch and English sentence pairs.\n",
        "Experiment with different parameters and see what happens to the evaluation metrics.\n",
        "Also make sure to look at the output since that can often times tell you more than an averaged metric.\n",
        "\n",
        "We have included an example to compare greedy and beam search.\n",
        "\n",
        "You have to adjust the parameters given to `run_inference`. It's up to you what to experiment with, some basic examples:\n",
        "1. Try out different temperature values, what do you see? How does this influence the evaluation metrics?\n",
        "2. Use different beam sizes, what happens to the time it takes to generate the candidates? What about the metrics? (Remember when we timed the Viterbi decoding in exercise 1? You can do something similar here.)\n",
        "3. Inspect some of the translations (you don't have to be able to speak Dutch, just look at the translations). Do you notice patterns? Common mistakes? Ungrammatical sentences? Can you think of some reasons of why this happens?\n",
        "\n",
        "The goal here is that you get an intuition about what's involved in this process and what the trade-offs are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUScViDbfXPA"
      },
      "outputs": [],
      "source": [
        "# Download the dataset\n",
        "dataset_path = Path(\"dev.txt\")\n",
        "if not dataset_path.exists():\n",
        "    !wget https://github.com/AssistantsLab/AssistantsLab-Replication/raw/refs/heads/main/evaluation/data/nl-en/dev.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnqlwHspfXPA"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(dataset_path, sep=\"\\t\", names=[\"source_lang\", \"target_lang\", \"en_txt\", \"nl_txt\"])\n",
        "# We prepare our input by adding the prompt to the source sequences\n",
        "df[\"source_txt\"] = df[\"nl_txt\"].apply(lambda s: f\"{prompt}{s}\")\n",
        "# Only select a subset of the examples, otherwise inference will take too long\n",
        "# Do not change or remove the random state, otherwise your results cannot be compared to others!\n",
        "df = df.sample(100, random_state=2)\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nyzQpJXsfXPA"
      },
      "outputs": [],
      "source": [
        "def run_inference(\n",
        "    source_texts: list[str],\n",
        "    model,\n",
        "    tokenizer,\n",
        "    decoding_strategy: str,\n",
        "    start_token_id: int,\n",
        "    end_token_id: int,\n",
        "    device: torch.device,\n",
        "    max_length: int,\n",
        "    beam_width: int,\n",
        "    temperature: int,\n",
        ") -> list[str]:\n",
        "    \"\"\"A wrapper function to generate translations for a list of texts.\"\"\"\n",
        "\n",
        "    encoded_results = []\n",
        "    for text in tqdm.tqdm(source_texts, desc=\"Running inference\"):\n",
        "        sample_ids = tokenizer.encode(text, return_tensors=\"pt\").to(device)\n",
        "        if decoding_strategy == \"greedy\":\n",
        "            generated_ids = greedy_search(\n",
        "                model,\n",
        "                sample_ids,\n",
        "                start_token_id,\n",
        "                end_token_id,\n",
        "                device,\n",
        "                max_length,\n",
        "            )\n",
        "            generated_ids = generated_ids.squeeze()\n",
        "        elif decoding_strategy == \"beam\":\n",
        "            beam_generated_options = beam_search(\n",
        "                tokenizer,\n",
        "                model,\n",
        "                sample_ids,\n",
        "                start_token_id,\n",
        "                end_token_id,\n",
        "                device,\n",
        "                beam_width,\n",
        "                temperature,\n",
        "                max_length,\n",
        "                verbose=False,\n",
        "            )\n",
        "            generated_ids = beam_generated_options[0].ids()\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown decoding strategy: {decoding_strategy}\")\n",
        "\n",
        "        encoded_results.append(generated_ids)\n",
        "\n",
        "    # Transform all token ids back to strings in one go (a bit faster)\n",
        "    results = tokenizer.batch_decode(encoded_results, skip_special_tokens=True)\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "znoaV_gAfXPA"
      },
      "outputs": [],
      "source": [
        "# Store the results in the dataframe for further analysis\n",
        "df[\"results_greedy\"] = run_inference(\n",
        "    source_texts=df[\"source_txt\"].tolist(),\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    decoding_strategy=\"greedy\",\n",
        "    start_token_id=start_token_id,\n",
        "    end_token_id=end_token_id,\n",
        "    device=device,\n",
        "    max_length=64,\n",
        "    beam_width=3,\n",
        "    temperature=1,\n",
        ")\n",
        "df[\"results_beam\"] = run_inference(\n",
        "    source_texts=df[\"source_txt\"].tolist(),\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    decoding_strategy=\"beam\",\n",
        "    start_token_id=start_token_id,\n",
        "    end_token_id=end_token_id,\n",
        "    device=device,\n",
        "    max_length=64,\n",
        "    beam_width=3,\n",
        "    temperature=1,\n",
        ")\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E0KUT0xpfXPB"
      },
      "outputs": [],
      "source": [
        "# This is just an example analysis, you should experiment with different parameters\n",
        "\n",
        "greedy = evaluate(df[\"results_greedy\"].tolist(), df[\"en_txt\"].tolist())\n",
        "beam = evaluate(df[\"results_beam\"].tolist(), df[\"en_txt\"].tolist())\n",
        "\n",
        "records = []\n",
        "for res, label in [(greedy, \"greedy\"), (beam, \"beam\")]:\n",
        "    for key, value in dataclasses.asdict(res).items():\n",
        "        rec = {\"method\": label, \"metric\": key, \"value\": value}\n",
        "        records.append(rec)\n",
        "\n",
        "df_metrics = pd.DataFrame().from_records(records)\n",
        "\n",
        "sns.barplot(df_metrics, x=\"metric\", y=\"value\", hue=\"method\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxBoVvq5fXPB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}