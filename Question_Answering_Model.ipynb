{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yanhan-ss/NLP/blob/main/Exercise_6_Question_Answering.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbKtOV5R7pG-"
      },
      "source": [
        "# Exercise 6: Question Answering\n",
        "\n",
        "This session is based on lecture 11 (Question Answering).\n",
        "\n",
        "Relevant SLP chapters:\n",
        "* Chapter 14 (except 14.1 and 14.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M4UTxLE-7wIW"
      },
      "source": [
        "## Extractive QA : SQuAD\n",
        "\n",
        "In this exercise, we will learn how to finetune a transformer model on question answering, specifically extractive question answering, which is to predict / select the answer to a question given some context.\n",
        "The benchmark dataset for extractive question answering is [SQuAD](https://huggingface.co/datasets/rajpurkar/squad).\n",
        "\n",
        "We will finetune BERT on this dataset. (You are also free to explore other models).\n",
        "\n",
        "Connecting to a GPU runtime in Google Colab is highly recommended for this exercise.\n",
        "\n",
        "This is a fill in the blank exercise, where you are expected to fill in parts of the code marked `# TODO: your code here`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jV583KjD2L9v"
      },
      "source": [
        "### Part 1: The dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZvvDPWh7wji"
      },
      "outputs": [],
      "source": [
        "!pip install -q \"transformers[torch]\" datasets evaluate \"accelerate\" pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4qEhU5P8K2a"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "\n",
        "import evaluate\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from datasets import (\n",
        "    load_dataset,\n",
        ")\n",
        "from IPython.display import HTML, display\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import (\n",
        "    AutoModelForQuestionAnswering,\n",
        "    AutoTokenizer,\n",
        "    DefaultDataCollator,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OHKF_FpN8OoA"
      },
      "outputs": [],
      "source": [
        "# we will only use a subset of the train split for squad for training because of limited compute and time\n",
        "# you can load more if you want and have the compute/time\n",
        "squad = load_dataset(\"squad\", split=\"train[:2500]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73HtzoWt8WLt"
      },
      "outputs": [],
      "source": [
        "squad.column_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvB3-kLEDofI"
      },
      "outputs": [],
      "source": [
        "# We'll turn the dataset in to a dataframe for easier inspection\n",
        "squad_df = squad.to_pandas()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slsCLBFoDofJ"
      },
      "outputs": [],
      "source": [
        "# Re-run this cell to see some other random examples\n",
        "display(HTML(squad_df.sample(10).to_html()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUOp-g6fDofK"
      },
      "source": [
        "If you look at the example from the dataset above, you have an example ID, the\n",
        "title, the context for extracting the answer, the question, and the answer with includes the index of the starting character of the answer in the context.\n",
        "The model will be train to predict these spans."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pou0CuWU2W4X"
      },
      "source": [
        "### Part 2: Preprocessing\n",
        "\n",
        "The SQuAD finetuning pipeline requires more preprocessing than the average text classification dataset. We will use the transformers tokenizers library that we briefly introduced in Exercise 1 to tokenize our text and preprocess the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5OP2gZm8he1"
      },
      "outputs": [],
      "source": [
        "model_id = \"bert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# these variables will become clear later\n",
        "doc_stride = 128\n",
        "max_token_length = 512\n",
        "squad_v2 = False\n",
        "pad_on_right = tokenizer.padding_side == \"right\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DF7kR2LN8j8O"
      },
      "source": [
        "Now one specific thing for the preprocessing in question answering is how to deal with very long documents.\n",
        "\n",
        "\n",
        "\n",
        "Some examples in a dataset may have a very long `context` that exceeds the maximum input length of the model. To deal with longer sequences, we can truncate (shorten) only the `context` by setting `truncation=\"only_second\"`.\n",
        "Next, we can map the start and end positions of the answer to the original `context` by setting `return_offset_mapping=True`.\n",
        "With the mapping in hand, now we can find the start and end tokens of the answer. We will use the `sequence_ids` method to find which part of the offset corresponds to the `question` and which corresponds to the `context`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAdcJCA64VEu"
      },
      "outputs": [],
      "source": [
        "example = None\n",
        "\n",
        "for ex in squad:\n",
        "    if len(tokenizer(ex[\"question\"], ex[\"context\"])[\"input_ids\"]) > max_token_length:\n",
        "        example = ex\n",
        "        break\n",
        "\n",
        "example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGTgpl0S58dA"
      },
      "source": [
        "Without any truncation, we get the following length for the input IDs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JieQMCJ57KC"
      },
      "outputs": [],
      "source": [
        "len(tokenizer(example[\"question\"], example[\"context\"])[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05QomrXh6B_e"
      },
      "source": [
        "Now, if we just truncate, we will lose information (and possibly the answer to our question):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUG-GyYy6EQv"
      },
      "outputs": [],
      "source": [
        "len(\n",
        "    tokenizer(\n",
        "        example[\"question\"],\n",
        "        example[\"context\"],\n",
        "        max_length=max_token_length,\n",
        "        truncation=\"only_second\",\n",
        "    )[\"input_ids\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8pYluzd7jPf"
      },
      "source": [
        "Removing part of the `context` might result in losing the answer we are looking for. To deal with this, we will allow one (long) example in our dataset to give several input features, each of length shorter than the maximum length of the model (or the one we set as a hyper-parameter). We can enable this functionality by setting `return_overflowing_tokens=True`. Also, just in case the answer lies at the point we split a long context, we allow some overlap between the features we generate controlled by the hyper-parameter `doc_stride`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60m5Lnoi6Bf2"
      },
      "outputs": [],
      "source": [
        "tokenized_example = tokenizer(\n",
        "    example[\"question\"],\n",
        "    example[\"context\"],\n",
        "    max_length=max_token_length,\n",
        "    truncation=\"only_second\",\n",
        "    return_overflowing_tokens=True,\n",
        "    stride=doc_stride,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9lkzNkN6ny2"
      },
      "source": [
        "Now we don't have one list of `input_ids`, but several:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_AQalPm06olu"
      },
      "outputs": [],
      "source": [
        "[len(x) for x in tokenized_example[\"input_ids\"]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rjucT1SD7wAt"
      },
      "source": [
        "And if we decode them, we can see the overlap:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CMFDJTx96tQp"
      },
      "outputs": [],
      "source": [
        "for x in tokenized_example[\"input_ids\"][:2]:\n",
        "    print(tokenizer.decode(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdEehOSS7LZv"
      },
      "source": [
        "We need to find in which of those words the answer actually is, and where exactly in that context.\n",
        "The models we will use require the start and end positions of these answers in the tokens, so we will also need to to map parts of the original context to some tokens.\n",
        "The tokenizer we're using can help us with that by returning an `offset_mapping`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mlfq6Tcp7TeH"
      },
      "outputs": [],
      "source": [
        "tokenized_example = tokenizer(\n",
        "    example[\"question\"],\n",
        "    example[\"context\"],\n",
        "    max_length=max_token_length,\n",
        "    truncation=\"only_second\",\n",
        "    return_overflowing_tokens=True,\n",
        "    return_offsets_mapping=True,\n",
        "    stride=doc_stride,\n",
        ")\n",
        "print(tokenized_example[\"offset_mapping\"][0][:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDviqFjh75di"
      },
      "source": [
        "This gives the corresponding start and end character in the original text for each token in our input IDs. The very first token (`[CLS]`) has (0, 0) because it doesn't correspond to any part of the question/answer, then the second token is the same as the characters 0 to 3 of the question:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hx0vw_K77dq"
      },
      "outputs": [],
      "source": [
        "first_token_id = tokenized_example[\"input_ids\"][0][1]\n",
        "offsets = tokenized_example[\"offset_mapping\"][0][1]\n",
        "print(\n",
        "    tokenizer.convert_ids_to_tokens([first_token_id])[0],\n",
        "    example[\"question\"][offsets[0] : offsets[1]],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C24hIXEL7_HB"
      },
      "source": [
        "So we can use this mapping to find the position of the start and end tokens of our answer in a given feature. We just have to distinguish which parts of the offsets correspond to the question and which part correspond to the context, this is where the `sequence_ids` method of our `tokenized_example` can be useful:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-oyUDsIm8BIX"
      },
      "outputs": [],
      "source": [
        "sequence_ids = tokenized_example.sequence_ids()\n",
        "print(sequence_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "neNplc6H8FFk"
      },
      "source": [
        "It returns `None` for the special tokens, then 0 or 1 depending on whether the corresponding token comes from the first sentence past (the question) or the second (the context). Now with all of this, we can find the first and last token of the answer in one of our input feature:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zr1wOLqC8LFY"
      },
      "outputs": [],
      "source": [
        "answers = example[\"answers\"]  # testing on one example\n",
        "start_char = answers[\"answer_start\"][0]\n",
        "end_char = start_char + len(answers[\"text\"][0])\n",
        "\n",
        "# TODO: your code here:\n",
        "# Initially, we select the entire span, your task is to find:\n",
        "# - token_start_index\n",
        "# - token_end_index\n",
        "# And correct them using the offset, which should result in:\n",
        "# - start_position\n",
        "# - end_position\n",
        "# In this part, we're only working with one example!\n",
        "\n",
        "# This is the full span, you have to some more processing!\n",
        "token_start_index = 0\n",
        "token_end_index = len(tokenized_example[\"input_ids\"][0]) - 1\n",
        "\n",
        "offsets = tokenized_example[\"offset_mapping\"][0]\n",
        "start_position = ...\n",
        "end_position = ...\n",
        "print(start_position, end_position)\n",
        "# This should print (13, 14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETaYIV5n8PpR"
      },
      "outputs": [],
      "source": [
        "print(tokenizer.decode(tokenized_example[\"input_ids\"][0][start_position : end_position + 1]))\n",
        "print(answers[\"text\"][0])\n",
        "# This sanity check should give 1565 both times!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0CXo7aG92UG"
      },
      "source": [
        "Now let's put everything together in one function we will apply to our training set. In the case of impossible answers (the answer is in another feature given by an example with a long context), we set the cls index for both the start and end position."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gy9HyVTZbpZR"
      },
      "outputs": [],
      "source": [
        "def preprocess(examples):\n",
        "    # Tokenize our examples with truncation and padding, but keep the overflows using a stride. This results\n",
        "    # in one example possible giving several features when a context is long, each of those features having a\n",
        "    # context that overlaps a bit the context of the previous feature.\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\"],\n",
        "        examples[\"context\"],\n",
        "        truncation=\"only_second\",\n",
        "        max_length=max_token_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
        "    # its corresponding example. This key gives us just that.\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "    # The offset mappings will give us a map from token to character position in the original context. This will\n",
        "    # help us compute the start_positions and end_positions.\n",
        "    offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n",
        "\n",
        "    # Let's label those examples!\n",
        "    tokenized_examples[\"start_positions\"] = []\n",
        "    tokenized_examples[\"end_positions\"] = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        # We will label impossible answers with the index of the CLS token.\n",
        "        input_ids = tokenized_examples[\"input_ids\"][i]\n",
        "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
        "\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "\n",
        "        # One example can give several spans, this is the index of the example containing this span of text.\n",
        "        sample_index = sample_mapping[i]\n",
        "        answers = examples[\"answers\"][sample_index]\n",
        "\n",
        "        # TODO: your code here\n",
        "        # get the start and end character of the answer\n",
        "        # get the start and end token indices, just as you've done above\n",
        "\n",
        "        token_start_index = ...\n",
        "        token_end_index = ...\n",
        "\n",
        "        # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n",
        "        if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n",
        "            tokenized_examples[\"start_positions\"].append(cls_index)\n",
        "            tokenized_examples[\"end_positions\"].append(cls_index)\n",
        "        else:\n",
        "            # TODO: your code here\n",
        "            # move the token_start_index and token_end_index to the two ends of the answer.\n",
        "            # note: we could go after the last offset if the answer is the last word (edge case).\n",
        "            # keep in mind to account for - 1 and + 1\n",
        "\n",
        "\n",
        "            tokenized_examples[\"start_positions\"].append(token_start_index)\n",
        "            tokenized_examples[\"end_positions\"].append(token_end_index)\n",
        "\n",
        "    return tokenized_examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYG99ANS_Vb8"
      },
      "source": [
        "This function works with one or several examples. In the case of several examples, the tokenizer will return a list of lists for each key:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NtpQPWdk_PK2"
      },
      "outputs": [],
      "source": [
        "features = preprocess(squad[:2])\n",
        "features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "fiQaOjvp8n4t"
      },
      "outputs": [],
      "source": [
        "# the datasets library has a .map() function that allows you to apply any function to each example in the dataset\n",
        "# it also allows for batching, so we can set batched=True for faster processing\n",
        "\n",
        "tokenized_squad = squad.map(preprocess, batched=True, remove_columns=squad.column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RU-6C9bWCYgx"
      },
      "source": [
        "### Part 3: Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VFz0pJZ08q0e"
      },
      "outputs": [],
      "source": [
        "# this will give some warnings you can ignore\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8I0utFrDLqA"
      },
      "outputs": [],
      "source": [
        "data_collator = DefaultDataCollator()\n",
        "train_loader = DataLoader(tokenized_squad, batch_size=16, shuffle=True, collate_fn=data_collator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyOwnb1wDkct"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "# move model over to detected device\n",
        "model.to(device)\n",
        "# initialize our optimizer\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=5e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zaUuuSpDs2N"
      },
      "outputs": [],
      "source": [
        "# set model to train mode\n",
        "model.train()\n",
        "\n",
        "n_epochs = 3\n",
        "\n",
        "# This will take about 15 minutes on a GPU, keep this in mind when doing to exercise!\n",
        "for epoch in range(n_epochs):\n",
        "    loop = tqdm(train_loader, leave=True)\n",
        "    for batch in loop:\n",
        "        # initialize calculated gradients (from prev step)\n",
        "        optim.zero_grad()\n",
        "        # pull all the tensor batches required for training\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        start_positions = batch[\"start_positions\"].to(device)\n",
        "        end_positions = batch[\"end_positions\"].to(device)\n",
        "        # train model on batch and return outputs (incl. loss)\n",
        "        outputs = model(\n",
        "            input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions\n",
        "        )\n",
        "        # extract loss\n",
        "        loss = outputs[0]\n",
        "        # calculate loss for every parameter that needs grad update\n",
        "        loss.backward()\n",
        "        # update parameters\n",
        "        optim.step()\n",
        "        # print relevant info to progress bar\n",
        "        loop.set_description(f\"Epoch {epoch}\")\n",
        "        loop.set_postfix(loss=loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSIiCv0b88u8"
      },
      "source": [
        "We will now perform inference on our trained model.\n",
        "Let's define our question and context:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hulurvjP89bW"
      },
      "outputs": [],
      "source": [
        "question = \"How many programming languages does BLOOM support?\"\n",
        "context = \"BLOOM has 176 billion parameters and can generate text in 46 languages natural languages and 13 programming languages.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iaz55L6G9FZE"
      },
      "outputs": [],
      "source": [
        "inputs = tokenizer(question, context, return_tensors=\"pt\").to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nzd1uSfA9G5t"
      },
      "outputs": [],
      "source": [
        "# we won't compute gradients since we only want to do inference\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)  # do a forward pass in the model\n",
        "    print(outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23YcbVZD9IOW"
      },
      "outputs": [],
      "source": [
        "answer_start_index = outputs.start_logits.argmax()\n",
        "answer_end_index = outputs.end_logits.argmax()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoDHRAbY9JVT"
      },
      "outputs": [],
      "source": [
        "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]  # get the tokenized answer\n",
        "tokenizer.decode(predict_answer_tokens)  # convert those tokens to readable text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ckp0-_Lii6L"
      },
      "source": [
        "#### Visualising the scores\n",
        "\n",
        "We can visualize tokens in the sequence and the scores the model assigns to them.\n",
        "With this, we have some idea of what the model is looking for in answering these questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tovXGdzbik3S"
      },
      "outputs": [],
      "source": [
        "# Use plot styling from seaborn and increase size\n",
        "sns.set_style(\"darkgrid\")\n",
        "plt.rcParams[\"figure.figsize\"] = (20, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgeicfVAjBzt"
      },
      "source": [
        "Retrieve all of the start and end scores, and use all of the tokens as x-axis labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsSMacKqim36"
      },
      "outputs": [],
      "source": [
        "# TODO your code here:\n",
        "# create a plot two subplots:\n",
        "#   1. tokens + token ids on the y-axis and the start scores on the x-axis\n",
        "#   1. tokens + token ids on the y-axis and the end scores on the x-axis\n",
        "# note: use `input_ids` and `outputs` you've used above\n",
        "# the tokenizer can convert them back to strings\n",
        "# make sure the tokens are unique...\n",
        "start_scores = ...\n",
        "end_scores = ...\n",
        "token_labels = ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaH0c1j_jF0k"
      },
      "source": [
        "Create a bar plot showing the score for every input word being the \"start\" word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ME3ySIwFi7ag"
      },
      "outputs": [],
      "source": [
        "# Create a barplot showing the start word score for all of the tokens.\n",
        "fig, ax = plt.subplots(1, 2)\n",
        "ax1 = sns.barplot(x=start_scores, y=token_labels, ax=ax[0])\n",
        "ax2 = sns.barplot(x=start_scores, y=token_labels, ax=ax[1], color=\"red\")\n",
        "\n",
        "ax1.grid(True)\n",
        "ax2.grid(True)\n",
        "\n",
        "ax1.set_title(\"Start Scores\")\n",
        "ax2.set_title(\"End Scores\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jmXkEshmr2M-"
      },
      "source": [
        "Try out some other examples.\n",
        "See how the scores change, try to come up with some hard examples.\n",
        "You can take inspiration from what we've seen in the course:\n",
        "* amibiguity: structural, lexical, etc.\n",
        "* spelling errors\n",
        "* non-standard text\n",
        "* ...\n",
        "\n",
        "Explore what's easy and what's hard for the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r8f1dsbMWeB0"
      },
      "source": [
        "### Part 4: Evaluation\n",
        "\n",
        "Extractive QA is harder to evaluate than just comparing labels; more processing is needed.\n",
        "We need to make sure we can map model predictions back to the context and compare this with the intended answer if we want to evaluate the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0Wf0KE_TuO2"
      },
      "outputs": [],
      "source": [
        "squad_val = load_dataset(\"squad\", split=\"validation[200:204]\")  # Again a smaller sample\n",
        "squad_val_tokenized = squad_val.map(preprocess, batched=True, remove_columns=squad_val.column_names)\n",
        "val_loader = DataLoader(squad_val_tokenized, batch_size=4, shuffle=False, collate_fn=DefaultDataCollator())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKGkNW2GZaSS"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(val_loader))\n",
        "batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(**batch)\n",
        "output.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_U_3HZtudxa"
      },
      "outputs": [],
      "source": [
        "batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzhr99zieHj3"
      },
      "outputs": [],
      "source": [
        "print((output.start_logits.shape, output.end_logits.shape))\n",
        "\n",
        "print(output.start_logits.argmax(-1))\n",
        "print(output.end_logits.argmax(-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYj5AR7teYFT"
      },
      "source": [
        "We have one logit for each feature and each token. The most obvious thing to predict an answer for each feature is to take the index for the maximum of the start logits as a start position and the index of the maximum of the end logits as an end position.\n",
        "\n",
        "This will work great in a lot of cases, but what if this prediction gives us something impossible: the start position could be greater than the end position, or point to a span of text in the question instead of the answer. In that case, we might want to look at the second best prediction to see if it gives a possible answer and select that instead.\n",
        "\n",
        "However, picking the second best answer is not as easy as picking the best one: is it the second best index in the start logits with the best index in the end logits? Or the best index in the start logits with the second best index in the end logits? And if that second best answer is not possible either, it gets even trickier for the third best answer.\n",
        "\n",
        "To classify our answers, we will use the score obtained by adding the start and end logits. We won't try to order all the possible answers and limit ourselves to with a hyper-parameter we call n_best_size. We'll pick the best indices in the start and end logits and gather all the answers this predicts. After checking if each one is valid, we will sort them by their score and keep the best one.\n",
        "\n",
        "The only point left is how to check a given span is inside the context (and not the question) and how to get back the text inside. To do this, we need to add two things to our validation features:\n",
        "\n",
        "* the ID of the example that generated the feature (since each example can generate several features, as seen before);\n",
        "* the offset mapping that will give us a map from token indices to character positions in the context.\n",
        "\n",
        "That's why we will re-process the validation set with the following function, slightly different from prepare_train_features:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Vk73HVxgW9D"
      },
      "outputs": [],
      "source": [
        "def prepare_validation_features(examples):\n",
        "    # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n",
        "    # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n",
        "    # left whitespace\n",
        "    examples[\"question\"] = [q.lstrip() for q in examples[\"question\"]]\n",
        "\n",
        "    # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n",
        "    # in one example possible giving several features when a context is long, each of those features having a\n",
        "    # context that overlaps a bit the context of the previous feature.\n",
        "    tokenized_examples = tokenizer(\n",
        "        examples[\"question\" if pad_on_right else \"context\"],\n",
        "        examples[\"context\" if pad_on_right else \"question\"],\n",
        "        truncation=\"only_second\" if pad_on_right else \"only_first\",\n",
        "        max_length=max_token_length,\n",
        "        stride=doc_stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\",\n",
        "    )\n",
        "\n",
        "    # Since one example might give us several features if it has a long context, we need a map from a feature to\n",
        "    # its corresponding example. This key gives us just that.\n",
        "    sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n",
        "\n",
        "    # We keep the example_id that gave us this feature and we will store the offset mappings.\n",
        "    tokenized_examples[\"example_id\"] = []\n",
        "\n",
        "    for i in range(len(tokenized_examples[\"input_ids\"])):\n",
        "        # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n",
        "        sequence_ids = tokenized_examples.sequence_ids(i)\n",
        "        context_index = 1 if pad_on_right else 0\n",
        "\n",
        "        # One example can give several spans, this is the index of the example containing this span of text.\n",
        "        sample_index = sample_mapping[i]\n",
        "        tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n",
        "\n",
        "        # Set to [] the offset_mapping that are not part of the context so it's easy to determine if a token\n",
        "        # position is part of the context or not.\n",
        "        tokenized_examples[\"offset_mapping\"][i] = [\n",
        "            (o if sequence_ids[k] == context_index else [])\n",
        "            for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n",
        "        ]\n",
        "\n",
        "    return tokenized_examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exWNA9kng3Zb"
      },
      "outputs": [],
      "source": [
        "validation_features = squad_val.map(\n",
        "    prepare_validation_features,\n",
        "    batched=True,\n",
        "    remove_columns=squad_val.column_names,\n",
        ")\n",
        "val_loader = DataLoader(validation_features, batch_size=4, shuffle=False, collate_fn=DefaultDataCollator())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4Fx4Nrf0GfO"
      },
      "outputs": [],
      "source": [
        "test_df = validation_features.to_pandas()\n",
        "test_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H8kq2kyNvdjL"
      },
      "outputs": [],
      "source": [
        "raw_predictions = {\n",
        "    \"start_logits\": [],\n",
        "    \"end_logits\": [],\n",
        "}\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "        batch.pop(\"offset_mapping\")\n",
        "        outputs = model(**batch)\n",
        "        # we unpack the batch here so it's easier to work with the examples later on\n",
        "        raw_predictions[\"start_logits\"].extend([ex for ex in outputs[\"start_logits\"].cpu()])\n",
        "        raw_predictions[\"end_logits\"].extend([ex for ex in outputs[\"end_logits\"].cpu()])\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhkIEWAk7XFd"
      },
      "outputs": [],
      "source": [
        "raw_predictions[\"start_logits\"][0].shape  # one sample instead of the batch size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_u6mBlf60z7S"
      },
      "source": [
        "We can now refine the test we had before: since we set `[]` in the offset mappings when it corresponds to a part of the question, it's easy to check if an answer is fully inside the context. We also eliminate very long answers from our considerations (with an hyper-parameter we can tune).\n",
        "\n",
        "This part is again on a single example, the full implementation is after this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Civ7NO7m0mOi"
      },
      "outputs": [],
      "source": [
        "max_answer_length = 30\n",
        "n_best_size = 20\n",
        "\n",
        "start_logits = output.start_logits[0].cpu()\n",
        "end_logits = output.end_logits[0].cpu()\n",
        "offset_mapping = validation_features[0][\"offset_mapping\"]\n",
        "# The first feature comes from the first example. For the more general case, we will need to be match the example_id to\n",
        "# an example index\n",
        "context = squad_val[0][\"context\"]\n",
        "\n",
        "# Gather the indices the best start/end logits:\n",
        "start_indexes = torch.argsort(start_logits)[-n_best_size - 1 :].tolist()\n",
        "end_indexes = torch.argsort(end_logits)[-n_best_size - 1 :].tolist()\n",
        "valid_answers = []\n",
        "for start_index in start_indexes:\n",
        "    for end_index in end_indexes:\n",
        "        # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
        "        # to part of the input_ids that are not in the context.\n",
        "        if (\n",
        "            start_index >= len(offset_mapping)\n",
        "            or end_index >= len(offset_mapping)\n",
        "            or len(offset_mapping[start_index]) == 0\n",
        "            or len(offset_mapping[end_index]) == 0\n",
        "        ):\n",
        "            continue\n",
        "\n",
        "        # TODO: your code here\n",
        "        # Ignore answers with a length that is either < 0 or > max_answer_length.\n",
        "\n",
        "\n",
        "        # TODO: your code here\n",
        "        # Check that the answer is inside the context and fill the variables.\n",
        "        if ...:\n",
        "            start_char = ...\n",
        "            end_char = ...\n",
        "            score = ...\n",
        "            text = ...\n",
        "            valid_answers.append({\"score\": score, \"text\": text})\n",
        "\n",
        "valid_answers = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n",
        "valid_answers = \"\\n\".join(str(item) for item in valid_answers)\n",
        "\n",
        "print(f'Question: {squad_val[0][\"question\"]}')\n",
        "print(f'Partial context: {squad_val[0][\"context\"][:200]}')\n",
        "print(f'Gold answers: {squad_val[0][\"answers\"]}')\n",
        "print(f\"Predictions:\\n{valid_answers}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWz6fsUy3jQn"
      },
      "source": [
        "Your code should give:\n",
        "\n",
        "```\n",
        "Question: Who had the best record in the NFC?\n",
        "Partial context: Despite waiving longtime running back DeAngelo Williams and losing top wide receiver Kelvin Benjamin to a torn ACL in the preseason, the Carolina Panthers had their best regular season in franchise hi\n",
        "Gold answers: {'text': ['Carolina Panthers', 'the Panthers', 'Carolina'], 'answer_start': [137, 695, 330]}\n",
        "Predictions:\n",
        "{'score': tensor(4.2731), 'text': 'Carolina Panthers'}\n",
        "{'score': tensor(1.9384), 'text': 'Panthers'}\n",
        "{'score': tensor(1.8279), 'text': 'the Carolina Panthers'}\n",
        "{'score': tensor(0.4810), 'text': 'Carolina'}\n",
        "{'score': tensor(-1.9641), 'text': 'the Carolina'}\n",
        "{'score': tensor(-2.3274), 'text': 'New Orleans Saints and the 2011 Green Bay Packers'}\n",
        "{'score': tensor(-2.3703), 'text': 'Panthers'}\n",
        "{'score': tensor(-2.7467), 'text': 'New Orleans Saints and the 2011 Green Bay Packers. With their NFC-best 15–1 regular season record, the Panthers'}\n",
        "{'score': tensor(-3.2434), 'text': 'the Panthers'}\n",
        "{'score': tensor(-3.2682), 'text': 'DeAngelo Williams and losing top wide receiver Kelvin Benjamin to a torn ACL in the preseason, the Carolina Panthers'}\n",
        "{'score': tensor(-3.5486), 'text': '2009 New Orleans Saints and the 2011 Green Bay Packers'}\n",
        "{'score': tensor(-3.5672), 'text': 'Green Bay Packers'}\n",
        "{'score': tensor(-3.6009), 'text': 'Carolina Panthers had their best regular season in franchise history'}\n",
        "{'score': tensor(-3.6009), 'text': 'New Orleans Saints'}\n",
        "{'score': tensor(-3.9680), 'text': '2009 New Orleans Saints and the 2011 Green Bay Packers. With their NFC-best 15–1 regular season record, the Panthers'}\n",
        "{'score': tensor(-3.9865), 'text': 'Green Bay Packers. With their NFC-best 15–1 regular season record, the Panthers'}\n",
        "{'score': tensor(-4.8222), 'text': '2009 New Orleans Saints'}\n",
        "{'score': tensor(-4.8484), 'text': 'New Orleans Saints and the 2011 Green Bay Packers. With their NFC-best 15–1'}\n",
        "{'score': tensor(-4.9472), 'text': '2011 Green Bay Packers'}\n",
        "{'score': tensor(-5.0479), 'text': 'New Orleans Saints and the 2011 Green Bay Packers. With their NFC-best 15–1 regular season record'}\n",
        "```\n",
        "\n",
        "\n",
        "As we mentioned in the code above, this was easy on the first feature because we knew it comes from the first example. For the other features, we will need a map between examples and their corresponding features. Also, since one example can give several features, we will need to gather together all the answers in all the features generated by a given example, then pick the best one. The following code builds a map from example index to its corresponding features indices:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0oZqBPJk3lR0"
      },
      "outputs": [],
      "source": [
        "examples = squad_val\n",
        "features = validation_features\n",
        "\n",
        "example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "features_per_example = collections.defaultdict(list)\n",
        "for i, feature in enumerate(features):\n",
        "    features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pUzb21a3xjk"
      },
      "source": [
        "The code above only keeps answers that are inside the context, we need to also grab the score for the impossible answer (which has start and end indices corresponding to the index of the CLS token). When one example gives several features, we have to predict the impossible answer when all the features give a high score to the impossible answer (since one feature could predict the impossible answer just because the answer isn't in the part of the context it has access too), which is why the score of the impossible answer for one example is the *minimum* of the scores for the impossible answer in each feature generated by the example.\n",
        "\n",
        "We then predict the impossible answer when that score is greater than the score of the best non-impossible answer. All combined together, this gives us this post-processing function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwNkQjRK30U_"
      },
      "outputs": [],
      "source": [
        "def postprocess_qa_predictions(\n",
        "    examples, features, start_logits_predictions, end_logits_predictions, n_best_size=20, max_answer_length=30\n",
        "):\n",
        "    # Build a map example to its corresponding features.\n",
        "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
        "    features_per_example = collections.defaultdict(list)\n",
        "    for i, feature in enumerate(features):\n",
        "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
        "\n",
        "    # The dictionaries we have to fill.\n",
        "    predictions = collections.OrderedDict()\n",
        "\n",
        "    print(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n",
        "\n",
        "    for example_index, example in enumerate(tqdm(examples)):\n",
        "        # Those are the indices of the features associated to the current example.\n",
        "        feature_indices = features_per_example[example_index]\n",
        "\n",
        "        min_null_score = None  # Only used if squad_v2 is True, assignment 5\n",
        "        valid_answers = []\n",
        "\n",
        "        context = example[\"context\"]\n",
        "        # Looping through all the features associated to the current example.\n",
        "        for feature_index in feature_indices:\n",
        "            # We grab the predictions of the model for this feature.\n",
        "            start_logits = start_logits_predictions[feature_index]\n",
        "            end_logits = end_logits_predictions[feature_index]\n",
        "            # This is what will allow us to map some the positions in our logits to span of texts in the original\n",
        "            # context.\n",
        "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
        "\n",
        "            # Update minimum null prediction.\n",
        "            cls_index = features[feature_index][\"input_ids\"].index(tokenizer.cls_token_id)\n",
        "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
        "            if min_null_score is None or min_null_score < feature_null_score:\n",
        "                min_null_score = feature_null_score\n",
        "\n",
        "            # Go through all possibilities for the `n_best_size` greater start and end logits.\n",
        "            start_indexes = torch.argsort(start_logits)[-n_best_size - 1 :].tolist()\n",
        "            end_indexes = torch.argsort(end_logits)[-n_best_size - 1 :].tolist()\n",
        "            for start_index in start_indexes:\n",
        "                for end_index in end_indexes:\n",
        "                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n",
        "                    # to part of the input_ids that are not in the context.\n",
        "                    if (\n",
        "                        start_index >= len(offset_mapping)\n",
        "                        or end_index >= len(offset_mapping)\n",
        "                        or len(offset_mapping[start_index]) == 0\n",
        "                        or len(offset_mapping[end_index]) == 0\n",
        "                    ):\n",
        "                        continue\n",
        "                    # TODO: your code here\n",
        "                    # include what you've done above on a single example on everything here\n",
        "                    score = ...\n",
        "                    text = ...\n",
        "                    valid_answers.append({\"score\": score, \"text\": text})\n",
        "\n",
        "        if len(valid_answers) > 0:\n",
        "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
        "        else:\n",
        "            # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n",
        "            # failure.\n",
        "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
        "\n",
        "        # Let's pick our final answer: the best one or the null answer (only for squad_v2)\n",
        "        if squad_v2:\n",
        "            answer = best_answer[\"text\"] if best_answer[\"score\"] > min_null_score else \"\"\n",
        "            predictions[example[\"id\"]] = answer\n",
        "        else:\n",
        "            predictions[example[\"id\"]] = best_answer[\"text\"]\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJlbi3tJ4BYz"
      },
      "outputs": [],
      "source": [
        "final_predictions = postprocess_qa_predictions(\n",
        "    squad_val,\n",
        "    validation_features,\n",
        "    raw_predictions[\"start_logits\"],\n",
        "    raw_predictions[\"end_logits\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lr4UIU8CAWbr"
      },
      "outputs": [],
      "source": [
        "# huggingface provides a way to compute pre-defined metrics\n",
        "# for convenience, we're using this here as well\n",
        "metric = evaluate.load(\"squad_v2\" if squad_v2 else \"squad\")\n",
        "if squad_v2:\n",
        "    formatted_predictions = [\n",
        "        {\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in final_predictions.items()\n",
        "    ]\n",
        "else:\n",
        "    formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in final_predictions.items()]\n",
        "references = [{\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in squad_val]\n",
        "metric.compute(predictions=formatted_predictions, references=references)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCUJ3juhWaEM"
      },
      "source": [
        "### Part 5: SQuAD v2\n",
        "\n",
        "After the release of SQuAD, SQuAD v2 was released.\n",
        "This version of the dataset combines the 100k questions from SQuAD v1 with an additional 50k unanswerable questions.\n",
        "This increases the complexity of the task: systems must not only answer questions, but also determine when no answer is supported by the paragraph and abstain from answering.\n",
        "\n",
        "Here, we're looking at two problems we have to fix before we can use SQuAD v2:\n",
        "\n",
        "- What additional pre-processing has to be done?\n",
        "- How do we evaluate this definition of the task?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oc4hXDGQXOFH"
      },
      "outputs": [],
      "source": [
        "squadv2 = load_dataset(\"squad_v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWIjSGr3cLt3"
      },
      "outputs": [],
      "source": [
        "# an example where the answer is not on the context\n",
        "squadv2[\"train\"][130318]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxC3v1KpXWsG"
      },
      "outputs": [],
      "source": [
        "# this will error because of empty answer spans\n",
        "features_v2 = preprocess(squadv2[\"train\"][130316:130318])\n",
        "\n",
        "# TODO: adapt the functions above for preprocessing so they can deal with empty answers"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "nlp-course",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
